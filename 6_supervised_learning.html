<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Supervised Learning</title>
    <meta charset="utf-8" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/pagedtable/css/pagedtable.css" rel="stylesheet" />
    <script src="libs/pagedtable/js/pagedtable.js"></script>
    <script src="libs/kePrint/kePrint.js"></script>
    <link href="libs/lightable/lightable.css" rel="stylesheet" />
    <link rel="stylesheet" href="style.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Supervised Learning

---




### Supervised Learning 

The agenda for this will be

* Basic Theory 

* Predict Survival on Titanic using Supervised Learning

--

.footnote[`WARNING: THERE WILL BE MATH`]

---

### The Supervised Learning &lt;/br&gt;Problem

Standard working assumption in supervised learning is that the mechanish that generates the data can be desribed as

$$ Y = f(X) + \epsilon $$
`\(f\)` is a function, `\(X=(X_1,X_2,\dots,X_p)\)` is a vector of inputs/features, `\(Y\)` is the labels/output/response and `\(\epsilon\)` is random noise.

--

#### General Problem:
We are interested in estimating the function `\(f\)` using a dataset consisting of labels: `\(Y\)` and features: `\(X\)`, that are generated by the same mechanish described above.

???

### Exercise 6.1.



---

### Example

A regression problem `\(f(x)=\sin(x)\)` where the noise `\(\epsilon\)` is normally distributed with mean `\(0\)` and standard deviation `\(0.4\)`. 

&lt;img src="images/supervised-learning/unnamed-chunk-3-1.png" width="80%" /&gt;

---

### The Supervised Learning&lt;/br&gt;Problem

The unknown `\(f\)` is estimated by: `\(\hat{f}\)` by minimizing the average loss/in-sample error/in-sample fit: `\(L(\hat{f}(x),y)\)` over the dataset: `\((x_1, y_1),(x_2,y_2),\dots,(x_N,y_N)\)`. 

`$$\frac{1}{N}\sum_{i=1}^N L(\hat{f}(x_i),y_i)$$`
Stated in another way, find a function `f` that minimizes: `mean(loss(f(x), y))`.

--
#### Regression and Classification

- Classification: `\(Y\)` is discrete. Binary classification refers to two classes and multiclass classification referes to multiple classes. 
Standard loss function is the 0-1 loss: `\(L(f(x),y)=I\{f(x)\neq y\}\)` and the cross-entropy loss: `\(−(ylog(f(x))+(1−y)log(1−f(x)))\)`.

- Regression: `\(Y\)` is continous. Standard loss functions are the squared loss: `\(L(f(x),y)=(f(x)- y)^2\)` and the absolute loss: `\(L(f(x),y)=|f(x)- y|\)`.

???

### Exercise 6.2.




---

### Example

A knn model with 1 neighbour gives the smalles possible in-sample error. But what happens if we try the model on new unseen data?
&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;&lt;/br&gt;
&lt;img src="images/supervised-learning/unnamed-chunk-5-1.png" width="80%" /&gt;

---

### Example

A knn model with 1 neighbour gives the smalles possible in-sample error. But what happens if we try the model on new unseen data?&lt;/br&gt;
&lt;i&gt;The model doesn't fit well to the new unseen data, intuitively because of the variability of the random noise which is added to the signal.&lt;/i&gt;

&lt;img src="images/supervised-learning/unnamed-chunk-6-1.png" width="80%" /&gt;

---

### Example

The nice in-sample fitting model generalizes poorly. Compared to an intuitively less complex model.

&lt;img src="images/supervised-learning/unnamed-chunk-7-1.png" width="80%" /&gt;

???

### Exercise 6.3.



---

### Important Distinction

What have we learned:


&lt;i&gt;Training loss/in-sample fit is not the best measure of model performance on new data, because we can easily &lt;b&gt;overfit&lt;/b&gt;: Fit a complex model well to data, which won't generalize to new data. &lt;/i&gt;


&lt;img src="images/supervised-learning/unnamed-chunk-9-1.png" width="80%" /&gt;

???

### Theoretical interpretation

.left-left[
* Bias is the simplifying assumptions made by the model to make the target function easier to approximate.
* Variance is the amount that the estimate of the target function will change given different training data.
* Trade-off is tension between the error introduced by the bias and the variance.

]
.pull-right[
![](biasvariance.png)
]

---

### Model Assesment and Selection&lt;/br&gt; with Resampling.

&lt;/br&gt;
&lt;i&gt;The generalization performance of a learning method relates to its prediction capability on independent test data. Assessment of this performance is extremely important in practice, since it guides the choice of learning method or model, and gives us a measure of the quality of the ultimately chosen model.&lt;/i&gt;
&lt;/br&gt;
&lt;/br&gt;
--

![](images/train_test_validation.png)


???

Training dataset which is ~50% of the data, validation- and test-dataset is ~25% each

* &lt;b&gt;Model Selection&lt;/b&gt;: estimating the performance of different models in order to choose the best one. 
  * This is done by training the model on the training set and calculating the loss on the validation set.

* &lt;b&gt;Model Assesment&lt;/b&gt;: having chosen a final model, estimating its prediction error (generalization error) on new data.
  * This is done when a final model has been chosen on the validation set. Then the model performance is assesed by calculating the loss on the test set. This performance measure is given to the end-user of the model.


---

### Validation Loss is close to&lt;/br&gt; Testing Loss

&lt;img src="images/supervised-learning/unnamed-chunk-10-1.png" width="80%" /&gt;

---

### Better use of data for&lt;/br&gt; validation

To make better use of the validation data one often uses cross-validation instead of splitting into train and validation.

&lt;center&gt;
  &lt;img src="images/train_vallidation.png"/&gt;
&lt;/center&gt;

---

### Cross-validation

To make better use of the validation data one often uses cross-validation instead of splitting into train and validation.


&lt;center&gt;
  &lt;img src="images/cross_validation.png" width="60%" height="60%"/&gt;  
&lt;/center&gt;


---

### Predict survival on Titanic

* Objective
  * Predict survival on the RMS Titanic
* Description
  * On April 15, 1912, during her maiden voyage, the RMS Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class. We will complete an analysis of what sorts of people were likely to survive and finally use the tools of machine learning to predict which passengers survived the tragedy.

&lt;center&gt;
  &lt;img src="images/intro.jpg" width=40% height=40%/&gt;
&lt;/center&gt;


---

### Dataset Example

- `Data` Train if training dataset, test if testing dataset&lt;/br&gt;
- `survival`	Survival	0 = No, 1 = Yes
- `pclass`	Ticket class	1 = 1st, 2 = 2nd, 3 = 3rd
- `sex`	Sex	
- `Age`	Age in years	
- `sibsp`	# of siblings / spouses aboard the Titanic	
- `parch`	# of parents / children aboard the Titanic	
- `ticket`	Ticket number	
- `fare`	Passenger fare	
- `cabin`	Cabin number	
- `embarked`	Port of Embarkation	C = Cherbourg, Q = Queenstown, S = Southampton

&lt;table class="table table-striped" style="font-size: 16px; width: auto !important; "&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; Age &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Cabin &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Fare &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Name &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Parch &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Pclass &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Sex &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; SibSp &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Survived &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 27 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; B71 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 52.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Davidson, Mrs. Thornton (Orian &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1st &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; female &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; NA &lt;/td&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 15.5000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; O'Brien, Mr. Thomas &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3rd &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; male &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 16 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 8.0500 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Sunderland, Mr. Victor Francis &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3rd &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; male &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 60 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt;  &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 39.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Brown, Mr. Thomas William Solom &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2nd &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; male &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 22 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; B36 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 61.9792 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Ostby, Miss. Helene Ragnhild &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1st &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; female &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 41 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; E40 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 134.5000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Burns, Miss. Elizabeth Margaret &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1st &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; female &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

### Exercise 6.1.

???
1. Make string variables into factors (Hint: Can `mutate_if(is.character, factor)` be used in a pipe an why?)

---

### Data Exploration




```r
train %&gt;% 
    mutate(Survived=ifelse(Survived==1, 'Yes', 'No')) %&gt;% 
    ggplot() +
    geom_bar(aes(x=SibSp + Parch, fill=Survived), position="dodge") +
    ggtitle('Impact of Family Size')
```

&lt;img src="images/supervised-learning/unnamed-chunk-13-1.png" width="70%" /&gt;

---

### Data Exploration

```r
train %&gt;% 
    mutate(Survived=ifelse(Survived==1, 'Yes', 'No')) %&gt;% 
    ggplot(aes(x= Age)) +
    geom_density(aes(color=Survived, fill=Survived), alpha=0.1) +
    facet_wrap(~Sex, nrow=2) +
    ggtitle('Impact of Age and Sex')
```

&lt;img src="images/supervised-learning/unnamed-chunk-14-1.png" width="70%" /&gt;

---

### Predict Survival

We will now be using XGBoost, a gradient boosting model to predict the survival of passenger. This model is widely used due to it's very good performance.

&lt;center&gt;  
  &lt;img src="images/xgboost.png"/&gt;
&lt;/center&gt;

* This type of model is labelled as one of the best "Off-the-Shelf" models for supervised learning as argued in *The Elements Of Statistical Learning* chapter 10. 

* It used in almost all winning solution in [Kaggle](https://www.kaggle.com/) a crowdsourced machine learning homepage.

---

### What is XGBoost?

.pull-left[
#### Classification and Regression Trees
We choose the variable and split-point to achieve the best fit. Then one or both of these regions are split into two more regions, and this process is continued, until some stopping rule is applied.
]
.pull-right[
![](images/recursive_partitioning.png)
]


---

### What is XGBoost?

.pull-left[
XGBoost is similar to the Random Forest model. A Random Forest builds a large collection of *de-correlated* classification or regression trees, and then averages then to yield a prediction.

For each tree one takes a subsample of the observations and during training one picks at random a couble of variables and uses the best one for the split.
]
.pull-right[
![](images/random_forest.png)
]

--

XGBoost is a *Stochastic Gradient Boosting* model. It trains each new tree in an additive manner and not independent of the other trees.
In each iteration it fits a new tree based on a subsample of the rows and columns of the data and adds it to the ensemble:

`$$f_t = f_{t-1} + \eta \cdot T$$`

The `\(\eta\)` is called a learning rate, it is a *regularization parameter*. 

--

This is a bit of a simplified explanation, one can read more in the [Original XGBoost Paper](https://arxiv.org/pdf/1603.02754.pdf).


---

### What is XGBoost?

.center[
&lt;img src="images/tree1.png" style = "width:auto; max-height:500px;"&gt;
]

???
- The sum of second order gradient of training data classified to the leaf. If it is square loss, this simply corresponds to the number of instances seen by a split or collected by a leaf during training. The deeper in the tree a node is, the lower this metric will be.

- Gain (for split nodes): the information gain metric of a split (corresponds to the importance of the node in the model).

- Value (for leafs): the margin value that the leaf may contribute to prediction.

- The tree root nodes also indicate the Tree index (0-based).

- The "Yes" branches are marked by the "&lt; split_value" label. The branches that also used for missing values are marked as bold (as in "carrying extra capacity").

---

### What is XGBoost?

.center[
&lt;img src="images/trees.png" style = "width:auto; max-height:500px;"&gt;
&lt;img src="images/trees.png" style = "width:auto; max-height:500px;"&gt;
&lt;img src="images/trees.png" style = "width:auto; max-height:500px;"&gt;
&lt;img src="images/trees.png" style = "width:auto; max-height:500px;"&gt;
&lt;img src="images/trees.png" style = "width:auto; max-height:500px;"&gt;
&lt;img src="images/trees.png" style = "width:auto; max-height:500px;"&gt;
]

---

### Dealing with Missing Values

Locate `NA` values.


```r
titanic %&gt;% summarise_all(list(sum_NA = ~sum(is.na(.))))
```

&lt;table class="table table-striped" style="font-size: 16px; width: auto !important; "&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; Age_sum_NA &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Cabin_sum_NA &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Fare_sum_NA &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Name_sum_NA &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Parch_sum_NA &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Pclass_sum_NA &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Sex_sum_NA &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; SibSp_sum_NA &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Survived_sum_NA &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 263 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

Replace `NA` values.


```r
titanic &lt;- titanic %&gt;% 
  mutate(
    Age = if_else(is.na(Age), mean(Age, na.rm = T), Age),
    Fare = if_else(is.na(Fare), mean(Fare, na.rm = T), Fare)
  ) 
```

---

### Feature Engineering

Extract numeric features.


```r
age_parch_sibsp &lt;- titanic %&gt;% 
  select(PassengerId, Age, Parch, SibSp)

survived_data &lt;- titanic %&gt;% 
  select(PassengerId, Survived, Data)
```

--

Make new features which might be predictive


```r
age_parch_sibsp &lt;- age_parch_sibsp %&gt;% 
  mutate(family_size = Parch + SibSp,
         avg_family_size = mean(Parch + SibSp))
```

---

### Feature Engineering

We will encode the `Pclass`, `Sex` and `Embarked` variable using &lt;i&gt;one-hot-encoding&lt;/i&gt;. We first create a dummy object which can be used later to one hot encode new data. 


```r
pclass_sex_embarked &lt;- titanic %&gt;%
  select(PassengerId, Pclass, Sex, Embarked)

dmy &lt;- pclass_sex_embarked %&gt;% 
  dummyVars(~ ., ., sep='_')
```

We then encode it using the `predict()` method.


```r
predict(dmy, pclass_sex_embarked)
```

&lt;table class="table table-striped" style="font-size: 16px; width: auto !important; "&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; PassengerId &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Pclass_1st &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Pclass_2nd &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Pclass_3rd &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Sex_female &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Sex_male &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Embarked_ &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Embarked_Cherbourg &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Embarked_Queenstown &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 91 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1070 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1233 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 352 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 221 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 45 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;


???

alternative way
model.data &lt;- data %&gt;%
  select(Survived, SibSp, Parch, Pclass, Sex, Age, Fare) %&gt;% 
  model.matrix(~ . - 1, 
               data=., 
               contrasts.arg = 
                 lapply(.[,sapply(.,is.factor)], contrasts, contrasts=FALSE)) %&gt;% 
  data.frame()

pclass_sex_embarked &lt;- titanic %&gt;%
  select(PassengerId, Pclass, Sex, Embarked) %&gt;% 
  gather(Pclass, Sex, Embarked, key = key, value = value) %&gt;% 
  unite('key', key, value) %&gt;% 
  mutate(value=1) %&gt;% 
  spread(key, value, fill=0)

pclass_sex_embarked %&gt;% 
  print_table()

---

### Exercise 6.2.

---

### Regular Expression



```r
library(stringr)

title &lt;- titanic %&gt;% 
  mutate(Title = str_extract(Name, '\\W\\w+\\.'))
```

&lt;img src="images/supervised-learning/unnamed-chunk-24-1.png" width="80%" /&gt;

???
1. Import `stringr` and have a look at `?str_match`. `str_match` implements regular expressions in R, these are very useful for extracting textual patterns from strings, and handy to know the existence of. One can easily learn regular expressions on nice source is https://regexone.com/.

---

### Putting the Features&lt;/br&gt; together


```r
titanic_preprocessed &lt;- age_parch_sibsp %&gt;% 
  left_join(survived_data, by = 'PassengerId') %&gt;% 
  left_join(pclass_sex_embarked, by = 'PassengerId') %&gt;% 
  left_join(title, by='PassengerId')

train &lt;- titanic_preprocessed %&gt;% 
  filter(Data == 'train') %&gt;% 
  select(-c(PassengerId, Data))

test &lt;- titanic_preprocessed %&gt;% 
  filter(Data == 'test') %&gt;% 
  select(-c(PassengerId, Data))
```

---

### Validate the XGBoost model


```r
X_train &lt;- train %&gt;% 
  select(-Survived)
y_train &lt;- train %&gt;% 
  select(Survived)

xg.labelledData = xgb.DMatrix(
  data=X_train %&gt;% as.matrix(), 
  label=y_train %&gt;% as.matrix()
)
```

--

#### XGBoost Parameters
Docs: https://xgboost.readthedocs.io/en/latest/index.html


```r
param &lt;- list("objective"           = "binary:logistic",  # We will train a binary classification model
              "booster"             = "gbtree",           # Specifying that the basis functions are regression trees
              "eta"                 = 0.01,               # Specifying the learning rate 0&lt;eta&lt;1
              "max_depth"           = 5,                  # Maximum depth of tree
              "subsample"           = 0.5,                # How much of the data is used in subsampling for a new iteration(rows)
              "colsample_bytree"    = 0.8                 # How much of the data is used in subsampling for a new iteration(collumns)
)
```
---

### Run a Cross Validation


```r
cvData &lt;- xgb.cv(params = param,
                 xg.labelledData,
                 nrounds = 500,
                 verbose = FALSE,
                 nfold = 5,
                 metrics = c("error"))
```

&lt;img src="images/supervised-learning/unnamed-chunk-29-1.png" width="70%" /&gt;

---

### Select optimal model


```r
cvData &lt;- xgb.cv(params = param,
                 xg.labelledData,
                 nrounds = 100,
                 verbose = FALSE,
                 nfold = 3,
                 metrics = c("error"))
```

--

Select the model which minimizes the CV error.

```r
optimal_number_of_trees &lt;- which.min(cvData$evaluation_log$test_error_mean)
```

--

Fit the model with the optimal number of trees to the entire training data.


```r
xgb.model &lt;- xgboost(
  params = param, 
  xg.labelledData, 
  nrounds = optimal_number_of_trees,
  verbose = FALSE
)
```

---

### Model Diagnostics

Feature importance plots.


```r
importance_matrix = xgb.importance(names(X_train), model = xgb.model)
xgb.plot.importance(importance_matrix, top_n = 20)
```

&lt;img src="images/supervised-learning/unnamed-chunk-33-1.png" width="70%" /&gt;

---

### Model Diagnostics
Plotting the trees.



![](images/tree1.png)








---

### Exercise 6.3.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
